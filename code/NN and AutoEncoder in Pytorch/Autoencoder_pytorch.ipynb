{"cells":[{"cell_type":"markdown","metadata":{"id":"BAEE51CAC7804597B5D0E06FB209A34B","trusted":true,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":true,"runtime":{"status":"default","execution_status":null},"scrolled":false,"notebookId":"6476be27ba5c1a7de46f2a36"},"source":"# Autoencoder"},{"cell_type":"markdown","metadata":{"id":"63D38FF1EF184969BC4C96A9881A9282","notebookId":"6476be27ba5c1a7de46f2a36","runtime":{"status":"default","execution_status":null},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"}},"source":"## 调包"},{"cell_type":"code","metadata":{"id":"3999AF60F2754394BA5F1A3C9AD8354B","trusted":true,"collapsed":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"scrolled":false,"notebookId":"6476be27ba5c1a7de46f2a36"},"source":"import os\nimport numpy as np\nfrom tqdm import tqdm\nimport torch\nimport torch.jit\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n#from openfe import openfe, transform\n#import graphviz\n\nfrom datetime import timedelta\nimport warnings; \nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd \nfrom pandas.tseries.offsets import Day, MonthEnd, MonthBegin # 时间处理\nimport numpy as np\nimport pyarrow.feather as feather\nimport matplotlib.pyplot as plt \n\n#import optuna\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, KBinsDiscretizer, LabelEncoder, OrdinalEncoder\n\n\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\n\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)\n","outputs":[{"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7f71e00ac4b0>"},"metadata":{},"execution_count":1}],"execution_count":1},{"cell_type":"markdown","metadata":{"id":"836EDAF884EB4FCD8FDC8CFCEC616059","notebookId":"6476be27ba5c1a7de46f2a36","runtime":{"status":"default","execution_status":null},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"}},"source":"## GPU"},{"cell_type":"code","metadata":{"id":"F36274A2996C4893A2B408BFE96F7385","notebookId":"6476be27ba5c1a7de46f2a36","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"if torch.cuda.is_available():\n    device = torch.device(\"cuda:0\")  # you can continue going on here, like cuda:1 cuda:2....etc. \n    print(\"Running on the GPU\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"Running on the CPU\")","outputs":[{"output_type":"stream","name":"stdout","text":"Running on the GPU\n"}],"execution_count":2},{"cell_type":"markdown","metadata":{"id":"5C430707480146F98EA713CA1E02F25F","notebookId":"6476be27ba5c1a7de46f2a36","runtime":{"status":"default","execution_status":null},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"}},"source":"## AutoEncoder"},{"cell_type":"markdown","metadata":{"id":"80539901613243758A421584FC74CAFE","notebookId":"6476be27ba5c1a7de46f2a36","runtime":{"status":"default","execution_status":null},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"}},"source":"## Model"},{"cell_type":"code","metadata":{"id":"78C32969BE8C47D58305A194F4D759EF","notebookId":"6476be27ba5c1a7de46f2a36","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"class Factor_data(Dataset):\n    def __init__(self, train_x, train_y): #默认输入的时候就已经是tensor\n        self.len = len(train_x)\n        self.x_data = train_x\n        self.y_data = train_y\n    def __getitem__(self, index):\n        return self.x_data[index], self.y_data[index]\n    def __len__(self):\n        return self.len","outputs":[],"execution_count":92},{"cell_type":"code","metadata":{"id":"216DA4AEC9F04AB897D67C8C56909BAE","notebookId":"6476be27ba5c1a7de46f2a36","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"class Factor_models(object):\n\n    def __init__(self,data,train_period,freq='m'):\n        '''\n        参数初始化\n        '''\n    \n        self.data = data\n        self.train_period = train_period\n        self.freq = freq # 训练集expanding的频率，是按月还是按年还是其他\n        \n    def predict_ret(self):\n        dates = self.data.index.unique()\n        dates = dates.sort_values()\n        print(dates)\n        test_dates= dates[self.train_period:len(dates)]\n        print(test_dates)\n        \n        # 创建一个train_end_list，训练集每月expanding。\n        preddf = pd.DataFrame() # 存储不同模型预测出来的y值，即存储样本外预测收益率的值\n        \n        \n        index = 0  # 计数用\n\n        for end_date in tqdm(test_dates,desc='Spilt and Train'): # 通过逐月改变训练集end_date的方法，切割样本\n            \n            temp = self.data[(self.data.index < end_date)]\n            valid_temp = self.data[self.data.index == end_date]\n            \n            # 训练集\n            train_y = temp.ret\n            train_x = temp.drop(['ret'], axis=1)\n    \n\n            # 测试集\n            #测试集为训练集的次月，只预测下个月的月度收益率\n            valid_y = valid_temp.ret\n            valid_x = valid_temp.drop(['ret'], axis=1)\n\n            #分割给optuna用的数据集\n            X_train, X_test, y_train, y_test = train_test_split(train_x, train_y, test_size=0.2, random_state=0)\n\n            \n            # 数据标准化\n            st = StandardScaler()\n            sv = StandardScaler()\n            s1 = StandardScaler()\n            s2 = StandardScaler()\n            \n            train_x = st.fit_transform(train_x)\n            valid_x = sv.fit_transform(valid_x)\n            X_train = s1.fit_transform(X_train)\n            X_test = s2.fit_transform(X_test)            \n\n            train_y = train_y.values.reshape(-1, 1)\n            valid_y = valid_y.values.reshape(-1,1)\n            y_train = y_train.values.reshape(-1,1)\n            y_test = y_test.values.reshape(-1,1)\n                                 \n            # 建模预测收益率\n            ## 先创建一个临时的temp_preddf,用来存储当前月份的验证集下的real y和不同模型的预测y\n            temp_preddf = pd.DataFrame() # 创建当前训练集下训练出的predict y和real y\n            temp_preddf['real_y'] = valid_y[:,0]# real_y就是验证集valid_y的第一列。因为valid_y是真实收益率数据在vailid_date上的切割\n            \n            ## NN3,未调参\n            ### 三层的神经网络       \n            train_x = torch.tensor(train_x,dtype = torch.float)\n            train_y = torch.tensor(train_y,dtype = torch.float)\n\n            valid_x = torch.tensor(valid_x,dtype = torch.float)\n            valid_y = torch.tensor(valid_y,dtype = torch.float)\n            \n            X_train = torch.tensor(X_train,dtype = torch.float)\n            X_test = torch.tensor(X_test,dtype = torch.float)\n            y_train = torch.tensor(y_train,dtype = torch.float)\n            y_test = torch.tensor(y_test,dtype = torch.float)\n\n            train_x =  train_x.to(device)\n            train_y = train_y.to(device)\n\n            valid_x = valid_x.to(device)\n            valid_y = valid_y.to(device) \n\n            X_train = X_train.to(device)\n            y_train = y_train.to(device)\n            X_test = X_test.to(device)\n            y_test = y_test.to(device) \n\n            \n            \n            batch_size = 40000\n            train_data = Factor_data(train_x,train_y) \n            train_loader = DataLoader(dataset = train_data,\n                                     batch_size = batch_size,\n                                     shuffle = False) #不打乱数据集\n            \n        \n            \n            # Create model\n            model = FactorAE().to(device)\n\n            #weight decay: 对所有weight参数进行L2正则化\n            weight_list,bias_list = [],[]\n            for name,p in model.named_parameters():\n                if 'bias' in name:\n                    bias_list += [p]\n                else:\n                    weight_list += [p]\n\n            optimizer = optim.SGD([{'params': weight_list, 'weight_decay':1e-5},{'params': bias_list, 'weight_decay':0}],lr = 1e-3,momentum = 0.9)\n\n            #Xavier Initialization\n            for n in model.modules():  \n                if isinstance(n,nn.Linear): #线性全连接层初始化 \n                    n.weight = nn.init.xavier_normal_(n.weight, gain=1.)   \n        \n            #training        \n            criterion = nn.MSELoss()\n            epoch_num = 10\n            loss_list = []\n            for epoch in range(epoch_num):\n                train_loss = 0.0\n\n                for data,y_return in train_loader:\n                    output = model(data,y_return)\n                    loss = criterion(output,y_return)\n                    optimizer.zero_grad()\n                    train_loss += loss\n                    loss.backward()\n                    optimizer.step()\n\n            predict_y = model(train_x,train_y)\n            predict_y = predict_y.detach().cpu().numpy()\n            temp_preddf['AEncoder_y'] = predict_y\n              \n            ## 将temp_preddf并入preddf\n            preddf = preddf.append(temp_preddf) # 将当前valid_date下得到的predict_y和real_y一起并入preddf中\n            self._preddf = preddf\n\n        torch.save(net, '/home/mw/project/NN/AEncoder_tuning.pt')\n        return preddf # 最后我们只返回preddf，也就是所有期的predict y和real y\n    \n    def cal_oos(self):\n        # 计算out-of-sample R2 根据代码开头的公式\n        try:\n            preddf = self._preddf # 如果self已经有self._preddf，即self.predict_ret()已经运行过了，已经预测过收益率了，则无需再次运行。\n        except:\n            preddf = self.predict_ret() # 如果之前没有运行过self.predict_ret()，则需要运行。\n        denominator = (preddf['real_y'] ** 2).sum() # 分母是真实收益率的平方和\n        numerator = preddf.apply(lambda x: preddf['real_y'] - x).iloc[:,1:] # 分子是real_y - predict_y的平方和\n        numerator = (numerator ** 2).sum()\n        \n        roos = 1 - numerator / denominator # 再用 1 减去分子/分母\n        roos.index = roos.index.str.rstrip('_y') # 之前的index都是模型_y，比如\"OLS_y\"，不美观，删除_y。\n        fig,ax = plt.subplots(figsize = (16,12)) # 画图，将不同模型的Roos画出来。\n        plt.title('Out-of-sample predicting R2', fontsize = 20)\n        ax.bar(x = roos.index, height = roos)\n        plt.show()\n        return roos # 返回样本外Roos，这个Roos是不同模型对应的样本外R2","outputs":[],"execution_count":93},{"cell_type":"markdown","metadata":{"id":"FB8F7750CADB438C83EC42C945922A60","notebookId":"6476be27ba5c1a7de46f2a36","runtime":{"status":"default","execution_status":null},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"}},"source":"## Main"},{"cell_type":"code","metadata":{"id":"E1125B388F5B45988C145D453DFE99FE","notebookId":"6476be27ba5c1a7de46f2a36","jupyter":{},"collapsed":false,"scrolled":true,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"#导入数据\nwith open('/home/mw/input/stock3636/chars60_rank_imputed.feather', 'rb') as f:\n    data = feather.read_feather(f)\ndata['date'] = data['date'].astype('datetime64')\ndata_3d = data.copy()\ndata.set_index('date', inplace=True)\n\n#data_3d.set_index([\"date\",\"permno\"], inplace=True)\n# 缺失值处理\n## 查看缺失值--没有缺失值\nprint('Missing data: {} items\\n'.format(len(data[data.isna().any(1)])), data[data.isna().any(1)]) # 看一下缺失值是哪些行\n\ns = (data.dtypes == 'int64')\nobject_cols = list(s[s].index)# 移除含有类别变量的列\n# 移除数据集含有类别变量的列（因为我们已经获得了含有独热编码的列）\ndata = data.drop(object_cols, axis=1)\ndata_3d = data_3d.drop(['gvkey','sic','ffi49'],axis=1)","outputs":[{"output_type":"stream","name":"stdout","text":"Missing data: 0 items\n Empty DataFrame\nColumns: [gvkey, permno, sic, ret, exchcd, shrcd, ffi49, lag_me, rank_acc, rank_cash, rank_maxret, rank_lgr, rank_roe, rank_sgr, rank_bm, rank_noa, rank_rdm, rank_chtx, rank_chpm, rank_cashdebt, rank_mom12m, rank_turn, rank_rvar_capm, rank_ato, rank_pscore, rank_chcsho, rank_beta, rank_dolvol, rank_alm, rank_std_dolvol, rank_rvar_mean, rank_rd_sale, rank_abr, rank_sp, rank_mom60m, rank_pctacc, rank_ill, rank_lev, rank_rna, rank_mom6m, rank_seas1a, rank_nincr, rank_mom1m, rank_zerotrade, rank_cfp, rank_pm, rank_me, rank_ni, rank_mom36m, rank_rvar_ff3, rank_gma, rank_roa, rank_rsup, rank_baspread, rank_sue, rank_grltnoa, rank_std_turn, rank_depr, rank_cinvest, rank_op, rank_agr, rank_ep, log_me]\nIndex: []\n\n[0 rows x 63 columns]\n"}],"execution_count":11},{"cell_type":"code","metadata":{"id":"C18B1E7EA9DD49568C4505611C0B905E","notebookId":"6476be27ba5c1a7de46f2a36","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"data","outputs":[{"output_type":"execute_result","data":{"text/plain":"                 ret  exchcd  shrcd        lag_me  rank_acc  rank_cash  \\\ndate                                                                     \n2013-07-31  0.106233     1.0   11.0  8.733313e+05  0.000000  -0.737624   \n2013-08-31  0.035066     1.0   11.0  9.501110e+05  0.000000  -0.694578   \n2013-09-30  0.089279     1.0   11.0  9.834276e+05  0.000000  -0.698404   \n2013-10-31  0.074094     1.0   11.0  1.081776e+06  0.000000  -0.535950   \n2013-11-30  0.066257     1.0   11.0  1.159195e+06  0.000000  -0.522464   \n...              ...     ...    ...           ...       ...        ...   \n2022-08-31 -0.112676     3.0   11.0  2.345556e+04 -0.444488   0.984442   \n2022-09-30 -0.142857     3.0   11.0  2.081268e+04 -0.455121   0.984395   \n2022-10-31 -0.152778     3.0   11.0  1.783944e+04 -0.455197   0.984296   \n2022-11-30 -0.004754     3.0   11.0  1.511397e+04  0.916506   0.979402   \n2022-12-31 -0.203865     3.0   11.0  1.504212e+04  0.918573   0.979288   \n\n            rank_maxret  rank_lgr  rank_roe  rank_sgr  ...  rank_baspread  \\\ndate                                                   ...                  \n2013-07-31    -0.156328  0.000000  0.000000  0.000000  ...      -0.228856   \n2013-08-31    -0.357430  0.000000  0.000000  0.000000  ...      -0.013203   \n2013-09-30    -0.508903  0.000000 -0.109434  0.000000  ...       0.363164   \n2013-10-31    -0.387013  0.000000 -0.115681  0.000000  ...      -0.698288   \n2013-11-30    -0.402050  0.000000 -0.050129  0.000000  ...       0.113014   \n...                 ...       ...       ...       ...  ...            ...   \n2022-08-31     0.712110  0.245117 -0.536906  0.011786  ...       0.739678   \n2022-09-30     0.697556  0.237414 -0.534483  0.018765  ...      -0.042951   \n2022-10-31     0.737638  0.231110 -0.534813  0.021938  ...      -0.092155   \n2022-11-30    -0.093878 -0.916487 -0.568280 -0.173990  ...      -0.264802   \n2022-12-31     0.564615 -0.915676 -0.569246 -0.151533  ...      -0.866295   \n\n            rank_sue  rank_grltnoa  rank_std_turn  rank_depr  rank_cinvest  \\\ndate                                                                         \n2013-07-31  0.000000      0.000000      -0.668317   0.000000      0.000000   \n2013-08-31  0.000000      0.000000      -0.408898   0.000000      0.000000   \n2013-09-30 -0.468880      0.000000      -0.401991   0.000000      0.000000   \n2013-10-31 -0.284507      0.000000       0.332380   0.000000      0.000000   \n2013-11-30 -0.249838      0.000000       0.249859   0.000000      0.000000   \n...              ...           ...            ...        ...           ...   \n2022-08-31  0.741955     -0.609195       0.973928   0.971420      0.962517   \n2022-09-30  0.736685     -0.612265       0.225547   0.971920      0.962426   \n2022-10-31  0.736937     -0.609950       0.071356   0.971693      0.961642   \n2022-11-30  0.132331     -0.635952      -0.534522  -0.972564      0.968094   \n2022-12-31  0.132388     -0.634946      -0.607269  -0.973056      0.968466   \n\n             rank_op  rank_agr   rank_ep     log_me  \ndate                                                 \n2013-07-31  0.000000  0.000000  0.000000  13.680070  \n2013-08-31  0.000000  0.000000  0.000000  13.764334  \n2013-09-30  0.000000  0.000000  0.000000  13.798799  \n2013-10-31  0.000000  0.000000  0.000000  13.894115  \n2013-11-30  0.000000  0.000000  0.000000  13.963237  \n...              ...       ...       ...        ...  \n2022-08-31 -0.660321 -0.597799 -0.793579  10.062863  \n2022-09-30 -0.657766 -0.596149 -0.818037   9.943318  \n2022-10-31 -0.652667 -0.605868 -0.803481   9.789167  \n2022-11-30 -0.895235  0.885776 -0.803432   9.623375  \n2022-12-31 -0.894276  0.888649 -0.784222   9.618609  \n\n[409280 rows x 59 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ret</th>\n      <th>exchcd</th>\n      <th>shrcd</th>\n      <th>lag_me</th>\n      <th>rank_acc</th>\n      <th>rank_cash</th>\n      <th>rank_maxret</th>\n      <th>rank_lgr</th>\n      <th>rank_roe</th>\n      <th>rank_sgr</th>\n      <th>...</th>\n      <th>rank_baspread</th>\n      <th>rank_sue</th>\n      <th>rank_grltnoa</th>\n      <th>rank_std_turn</th>\n      <th>rank_depr</th>\n      <th>rank_cinvest</th>\n      <th>rank_op</th>\n      <th>rank_agr</th>\n      <th>rank_ep</th>\n      <th>log_me</th>\n    </tr>\n    <tr>\n      <th>date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2013-07-31</th>\n      <td>0.106233</td>\n      <td>1.0</td>\n      <td>11.0</td>\n      <td>8.733313e+05</td>\n      <td>0.000000</td>\n      <td>-0.737624</td>\n      <td>-0.156328</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>-0.228856</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-0.668317</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>13.680070</td>\n    </tr>\n    <tr>\n      <th>2013-08-31</th>\n      <td>0.035066</td>\n      <td>1.0</td>\n      <td>11.0</td>\n      <td>9.501110e+05</td>\n      <td>0.000000</td>\n      <td>-0.694578</td>\n      <td>-0.357430</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>-0.013203</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-0.408898</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>13.764334</td>\n    </tr>\n    <tr>\n      <th>2013-09-30</th>\n      <td>0.089279</td>\n      <td>1.0</td>\n      <td>11.0</td>\n      <td>9.834276e+05</td>\n      <td>0.000000</td>\n      <td>-0.698404</td>\n      <td>-0.508903</td>\n      <td>0.000000</td>\n      <td>-0.109434</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.363164</td>\n      <td>-0.468880</td>\n      <td>0.000000</td>\n      <td>-0.401991</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>13.798799</td>\n    </tr>\n    <tr>\n      <th>2013-10-31</th>\n      <td>0.074094</td>\n      <td>1.0</td>\n      <td>11.0</td>\n      <td>1.081776e+06</td>\n      <td>0.000000</td>\n      <td>-0.535950</td>\n      <td>-0.387013</td>\n      <td>0.000000</td>\n      <td>-0.115681</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>-0.698288</td>\n      <td>-0.284507</td>\n      <td>0.000000</td>\n      <td>0.332380</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>13.894115</td>\n    </tr>\n    <tr>\n      <th>2013-11-30</th>\n      <td>0.066257</td>\n      <td>1.0</td>\n      <td>11.0</td>\n      <td>1.159195e+06</td>\n      <td>0.000000</td>\n      <td>-0.522464</td>\n      <td>-0.402050</td>\n      <td>0.000000</td>\n      <td>-0.050129</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.113014</td>\n      <td>-0.249838</td>\n      <td>0.000000</td>\n      <td>0.249859</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>13.963237</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2022-08-31</th>\n      <td>-0.112676</td>\n      <td>3.0</td>\n      <td>11.0</td>\n      <td>2.345556e+04</td>\n      <td>-0.444488</td>\n      <td>0.984442</td>\n      <td>0.712110</td>\n      <td>0.245117</td>\n      <td>-0.536906</td>\n      <td>0.011786</td>\n      <td>...</td>\n      <td>0.739678</td>\n      <td>0.741955</td>\n      <td>-0.609195</td>\n      <td>0.973928</td>\n      <td>0.971420</td>\n      <td>0.962517</td>\n      <td>-0.660321</td>\n      <td>-0.597799</td>\n      <td>-0.793579</td>\n      <td>10.062863</td>\n    </tr>\n    <tr>\n      <th>2022-09-30</th>\n      <td>-0.142857</td>\n      <td>3.0</td>\n      <td>11.0</td>\n      <td>2.081268e+04</td>\n      <td>-0.455121</td>\n      <td>0.984395</td>\n      <td>0.697556</td>\n      <td>0.237414</td>\n      <td>-0.534483</td>\n      <td>0.018765</td>\n      <td>...</td>\n      <td>-0.042951</td>\n      <td>0.736685</td>\n      <td>-0.612265</td>\n      <td>0.225547</td>\n      <td>0.971920</td>\n      <td>0.962426</td>\n      <td>-0.657766</td>\n      <td>-0.596149</td>\n      <td>-0.818037</td>\n      <td>9.943318</td>\n    </tr>\n    <tr>\n      <th>2022-10-31</th>\n      <td>-0.152778</td>\n      <td>3.0</td>\n      <td>11.0</td>\n      <td>1.783944e+04</td>\n      <td>-0.455197</td>\n      <td>0.984296</td>\n      <td>0.737638</td>\n      <td>0.231110</td>\n      <td>-0.534813</td>\n      <td>0.021938</td>\n      <td>...</td>\n      <td>-0.092155</td>\n      <td>0.736937</td>\n      <td>-0.609950</td>\n      <td>0.071356</td>\n      <td>0.971693</td>\n      <td>0.961642</td>\n      <td>-0.652667</td>\n      <td>-0.605868</td>\n      <td>-0.803481</td>\n      <td>9.789167</td>\n    </tr>\n    <tr>\n      <th>2022-11-30</th>\n      <td>-0.004754</td>\n      <td>3.0</td>\n      <td>11.0</td>\n      <td>1.511397e+04</td>\n      <td>0.916506</td>\n      <td>0.979402</td>\n      <td>-0.093878</td>\n      <td>-0.916487</td>\n      <td>-0.568280</td>\n      <td>-0.173990</td>\n      <td>...</td>\n      <td>-0.264802</td>\n      <td>0.132331</td>\n      <td>-0.635952</td>\n      <td>-0.534522</td>\n      <td>-0.972564</td>\n      <td>0.968094</td>\n      <td>-0.895235</td>\n      <td>0.885776</td>\n      <td>-0.803432</td>\n      <td>9.623375</td>\n    </tr>\n    <tr>\n      <th>2022-12-31</th>\n      <td>-0.203865</td>\n      <td>3.0</td>\n      <td>11.0</td>\n      <td>1.504212e+04</td>\n      <td>0.918573</td>\n      <td>0.979288</td>\n      <td>0.564615</td>\n      <td>-0.915676</td>\n      <td>-0.569246</td>\n      <td>-0.151533</td>\n      <td>...</td>\n      <td>-0.866295</td>\n      <td>0.132388</td>\n      <td>-0.634946</td>\n      <td>-0.607269</td>\n      <td>-0.973056</td>\n      <td>0.968466</td>\n      <td>-0.894276</td>\n      <td>0.888649</td>\n      <td>-0.784222</td>\n      <td>9.618609</td>\n    </tr>\n  </tbody>\n</table>\n<p>409280 rows × 59 columns</p>\n</div>"},"metadata":{},"execution_count":12}],"execution_count":12},{"cell_type":"code","metadata":{"id":"4156BFF436184F188F798989E7279C53","notebookId":"6476be27ba5c1a7de46f2a36","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"data_3d = data_3d.pivot_table(index=[\"date\",\"permno\"])\n#data_3d","outputs":[],"execution_count":13},{"cell_type":"code","metadata":{"id":"25E8AA6BA7FD43A785D6536E3C4F9225","notebookId":"6476be27ba5c1a7de46f2a36","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"data_3d","outputs":[{"output_type":"execute_result","data":{"text/plain":"                   exchcd        lag_me     log_me  rank_abr  rank_acc  \\\ndate       permno                                                        \n2013-05-31 87604      3.0  1.855514e+03   7.525917  1.000000  1.000000   \n           90640      3.0  1.416794e+04   9.558737 -1.000000 -1.000000   \n2013-06-30 10025      3.0  4.672376e+05  13.054593  0.274194  0.000000   \n           10182      3.0  1.905212e+06  14.460104 -0.879032  0.000000   \n           10259      3.0  1.567256e+05  11.962251  0.919355  0.000000   \n...                   ...           ...        ...       ...       ...   \n2022-12-31 93423      1.0  2.003252e+06  14.510282  0.639278 -0.084917   \n           93426      1.0  5.091941e+05  13.140584  0.473416  0.222179   \n           93429      3.0  1.345544e+07  16.414894  0.182905 -0.050019   \n           93434      3.0  4.731153e+04  10.764509  0.929789 -0.640946   \n           93436      3.0  6.148143e+08  20.236831 -0.010430  0.184180   \n\n                   rank_agr  rank_alm  rank_ato  rank_baspread  rank_beta  \\\ndate       permno                                                           \n2013-05-31 87604   1.000000 -1.000000 -1.000000      -1.000000  -1.000000   \n           90640  -1.000000  1.000000  1.000000       1.000000   1.000000   \n2013-06-30 10025   0.000000  0.254098  0.000000      -0.405622  -0.469880   \n           10182   0.000000  0.918033  0.000000      -0.839357  -0.566265   \n           10259   0.000000  0.622951  0.000000       0.614458   0.012048   \n...                     ...       ...       ...            ...        ...   \n2022-12-31 93423  -0.543784 -0.306801 -0.189940       0.003798   0.736497   \n           93426   0.005946 -0.054351  0.105228       0.056470  -0.191822   \n           93429  -0.243784 -0.944758 -0.234282      -0.706255  -0.699142   \n           93434  -0.644865  0.615088 -0.297816      -0.304634   0.669359   \n           93436   0.587568 -0.792694  0.598279       0.318815   0.603231   \n\n                   ...  rank_seas1a  rank_sgr   rank_sp  rank_std_dolvol  \\\ndate       permno  ...                                                     \n2013-05-31 87604   ...     1.000000 -1.000000  1.000000         1.000000   \n           90640   ...    -1.000000  1.000000 -1.000000        -1.000000   \n2013-06-30 10025   ...     0.000000  0.000000 -0.132530         0.028112   \n           10182   ...     0.000000  0.000000  0.983936         0.180723   \n           10259   ...     0.000000  0.000000  0.437751         0.389558   \n...                ...          ...       ...       ...              ...   \n2022-12-31 93423   ...     0.869702  0.714201  0.242233        -0.268551   \n           93426   ...     0.538462  0.165228  0.176794         0.165068   \n           93429   ...    -0.021978  0.884489 -0.546329        -0.376577   \n           93434   ...    -0.425432 -0.851146  0.558427         0.592125   \n           93436   ...    -0.463632  0.596904 -0.740445        -1.000000   \n\n                   rank_std_turn  rank_sue  rank_turn  rank_zerotrade  \\\ndate       permno                                                       \n2013-05-31 87604        1.000000 -1.000000   1.000000       -1.000000   \n           90640       -1.000000  1.000000  -1.000000        1.000000   \n2013-06-30 10025       -0.421687  0.000000  -0.225806        0.044534   \n           10182        0.068273  0.000000   0.137097        0.165992   \n           10259        0.397590  0.000000  -0.209677       -0.319838   \n...                          ...       ...        ...             ...   \n2022-12-31 93423        0.779404 -0.299054   0.896071       -0.738318   \n           93426       -0.575467  0.732861  -0.663878       -0.089164   \n           93429       -0.032307  0.164894  -0.066160       -0.755999   \n           93434       -0.832408  0.382979  -0.830672        0.867643   \n           93436        0.434629  0.688534   0.752091       -0.885325   \n\n                        ret  shrcd  \ndate       permno                   \n2013-05-31 87604   0.024691   11.0  \n           90640   0.449393   11.0  \n2013-06-30 10025  -0.108888   11.0  \n           10182  -0.060455   11.0  \n           10259   0.100218   11.0  \n...                     ...    ...  \n2022-12-31 93423  -0.034869   11.0  \n           93426  -0.047326   11.0  \n           93429  -0.010801   11.0  \n           93434   0.342342   11.0  \n           93436  -0.367334   11.0  \n\n[409280 rows x 59 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>exchcd</th>\n      <th>lag_me</th>\n      <th>log_me</th>\n      <th>rank_abr</th>\n      <th>rank_acc</th>\n      <th>rank_agr</th>\n      <th>rank_alm</th>\n      <th>rank_ato</th>\n      <th>rank_baspread</th>\n      <th>rank_beta</th>\n      <th>...</th>\n      <th>rank_seas1a</th>\n      <th>rank_sgr</th>\n      <th>rank_sp</th>\n      <th>rank_std_dolvol</th>\n      <th>rank_std_turn</th>\n      <th>rank_sue</th>\n      <th>rank_turn</th>\n      <th>rank_zerotrade</th>\n      <th>ret</th>\n      <th>shrcd</th>\n    </tr>\n    <tr>\n      <th>date</th>\n      <th>permno</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">2013-05-31</th>\n      <th>87604</th>\n      <td>3.0</td>\n      <td>1.855514e+03</td>\n      <td>7.525917</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>-1.000000</td>\n      <td>-1.000000</td>\n      <td>-1.000000</td>\n      <td>-1.000000</td>\n      <td>...</td>\n      <td>1.000000</td>\n      <td>-1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>-1.000000</td>\n      <td>1.000000</td>\n      <td>-1.000000</td>\n      <td>0.024691</td>\n      <td>11.0</td>\n    </tr>\n    <tr>\n      <th>90640</th>\n      <td>3.0</td>\n      <td>1.416794e+04</td>\n      <td>9.558737</td>\n      <td>-1.000000</td>\n      <td>-1.000000</td>\n      <td>-1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>...</td>\n      <td>-1.000000</td>\n      <td>1.000000</td>\n      <td>-1.000000</td>\n      <td>-1.000000</td>\n      <td>-1.000000</td>\n      <td>1.000000</td>\n      <td>-1.000000</td>\n      <td>1.000000</td>\n      <td>0.449393</td>\n      <td>11.0</td>\n    </tr>\n    <tr>\n      <th rowspan=\"3\" valign=\"top\">2013-06-30</th>\n      <th>10025</th>\n      <td>3.0</td>\n      <td>4.672376e+05</td>\n      <td>13.054593</td>\n      <td>0.274194</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.254098</td>\n      <td>0.000000</td>\n      <td>-0.405622</td>\n      <td>-0.469880</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-0.132530</td>\n      <td>0.028112</td>\n      <td>-0.421687</td>\n      <td>0.000000</td>\n      <td>-0.225806</td>\n      <td>0.044534</td>\n      <td>-0.108888</td>\n      <td>11.0</td>\n    </tr>\n    <tr>\n      <th>10182</th>\n      <td>3.0</td>\n      <td>1.905212e+06</td>\n      <td>14.460104</td>\n      <td>-0.879032</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.918033</td>\n      <td>0.000000</td>\n      <td>-0.839357</td>\n      <td>-0.566265</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.983936</td>\n      <td>0.180723</td>\n      <td>0.068273</td>\n      <td>0.000000</td>\n      <td>0.137097</td>\n      <td>0.165992</td>\n      <td>-0.060455</td>\n      <td>11.0</td>\n    </tr>\n    <tr>\n      <th>10259</th>\n      <td>3.0</td>\n      <td>1.567256e+05</td>\n      <td>11.962251</td>\n      <td>0.919355</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.622951</td>\n      <td>0.000000</td>\n      <td>0.614458</td>\n      <td>0.012048</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.437751</td>\n      <td>0.389558</td>\n      <td>0.397590</td>\n      <td>0.000000</td>\n      <td>-0.209677</td>\n      <td>-0.319838</td>\n      <td>0.100218</td>\n      <td>11.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">2022-12-31</th>\n      <th>93423</th>\n      <td>1.0</td>\n      <td>2.003252e+06</td>\n      <td>14.510282</td>\n      <td>0.639278</td>\n      <td>-0.084917</td>\n      <td>-0.543784</td>\n      <td>-0.306801</td>\n      <td>-0.189940</td>\n      <td>0.003798</td>\n      <td>0.736497</td>\n      <td>...</td>\n      <td>0.869702</td>\n      <td>0.714201</td>\n      <td>0.242233</td>\n      <td>-0.268551</td>\n      <td>0.779404</td>\n      <td>-0.299054</td>\n      <td>0.896071</td>\n      <td>-0.738318</td>\n      <td>-0.034869</td>\n      <td>11.0</td>\n    </tr>\n    <tr>\n      <th>93426</th>\n      <td>1.0</td>\n      <td>5.091941e+05</td>\n      <td>13.140584</td>\n      <td>0.473416</td>\n      <td>0.222179</td>\n      <td>0.005946</td>\n      <td>-0.054351</td>\n      <td>0.105228</td>\n      <td>0.056470</td>\n      <td>-0.191822</td>\n      <td>...</td>\n      <td>0.538462</td>\n      <td>0.165228</td>\n      <td>0.176794</td>\n      <td>0.165068</td>\n      <td>-0.575467</td>\n      <td>0.732861</td>\n      <td>-0.663878</td>\n      <td>-0.089164</td>\n      <td>-0.047326</td>\n      <td>11.0</td>\n    </tr>\n    <tr>\n      <th>93429</th>\n      <td>3.0</td>\n      <td>1.345544e+07</td>\n      <td>16.414894</td>\n      <td>0.182905</td>\n      <td>-0.050019</td>\n      <td>-0.243784</td>\n      <td>-0.944758</td>\n      <td>-0.234282</td>\n      <td>-0.706255</td>\n      <td>-0.699142</td>\n      <td>...</td>\n      <td>-0.021978</td>\n      <td>0.884489</td>\n      <td>-0.546329</td>\n      <td>-0.376577</td>\n      <td>-0.032307</td>\n      <td>0.164894</td>\n      <td>-0.066160</td>\n      <td>-0.755999</td>\n      <td>-0.010801</td>\n      <td>11.0</td>\n    </tr>\n    <tr>\n      <th>93434</th>\n      <td>3.0</td>\n      <td>4.731153e+04</td>\n      <td>10.764509</td>\n      <td>0.929789</td>\n      <td>-0.640946</td>\n      <td>-0.644865</td>\n      <td>0.615088</td>\n      <td>-0.297816</td>\n      <td>-0.304634</td>\n      <td>0.669359</td>\n      <td>...</td>\n      <td>-0.425432</td>\n      <td>-0.851146</td>\n      <td>0.558427</td>\n      <td>0.592125</td>\n      <td>-0.832408</td>\n      <td>0.382979</td>\n      <td>-0.830672</td>\n      <td>0.867643</td>\n      <td>0.342342</td>\n      <td>11.0</td>\n    </tr>\n    <tr>\n      <th>93436</th>\n      <td>3.0</td>\n      <td>6.148143e+08</td>\n      <td>20.236831</td>\n      <td>-0.010430</td>\n      <td>0.184180</td>\n      <td>0.587568</td>\n      <td>-0.792694</td>\n      <td>0.598279</td>\n      <td>0.318815</td>\n      <td>0.603231</td>\n      <td>...</td>\n      <td>-0.463632</td>\n      <td>0.596904</td>\n      <td>-0.740445</td>\n      <td>-1.000000</td>\n      <td>0.434629</td>\n      <td>0.688534</td>\n      <td>0.752091</td>\n      <td>-0.885325</td>\n      <td>-0.367334</td>\n      <td>11.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>409280 rows × 59 columns</p>\n</div>"},"metadata":{},"execution_count":70}],"execution_count":70},{"cell_type":"code","metadata":{"id":"120F741E278B4ECEB94368F1A123FE40","notebookId":"6476be27ba5c1a7de46f2a36","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"\"\"\"\ncol_name_3d = list(data_3d)\ncol_name_3d.remove('date')\ncol_name_3d.remove('permno')\ncol_name_3d\n\"\"\"","outputs":[],"execution_count":41},{"cell_type":"code","metadata":{"id":"D9CCEB6320F14E329B1D6FEE915018F3","notebookId":"6476be27ba5c1a7de46f2a36","jupyter":{},"collapsed":false,"scrolled":true,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"col_name_3d = data_3d.columns\ncol_3d = np.setdiff1d(col_name_3d, ['permno', 'date'])\nprint(col_3d)","outputs":[{"output_type":"stream","name":"stdout","text":"['exchcd' 'lag_me' 'log_me' 'rank_abr' 'rank_acc' 'rank_agr' 'rank_alm'\n 'rank_ato' 'rank_baspread' 'rank_beta' 'rank_bm' 'rank_cash'\n 'rank_cashdebt' 'rank_cfp' 'rank_chcsho' 'rank_chpm' 'rank_chtx'\n 'rank_cinvest' 'rank_depr' 'rank_dolvol' 'rank_ep' 'rank_gma'\n 'rank_grltnoa' 'rank_ill' 'rank_lev' 'rank_lgr' 'rank_maxret' 'rank_me'\n 'rank_mom12m' 'rank_mom1m' 'rank_mom36m' 'rank_mom60m' 'rank_mom6m'\n 'rank_ni' 'rank_nincr' 'rank_noa' 'rank_op' 'rank_pctacc' 'rank_pm'\n 'rank_pscore' 'rank_rd_sale' 'rank_rdm' 'rank_rna' 'rank_roa' 'rank_roe'\n 'rank_rsup' 'rank_rvar_capm' 'rank_rvar_ff3' 'rank_rvar_mean'\n 'rank_seas1a' 'rank_sgr' 'rank_sp' 'rank_std_dolvol' 'rank_std_turn'\n 'rank_sue' 'rank_turn' 'rank_zerotrade' 'ret' 'shrcd']\n"}],"execution_count":4},{"cell_type":"code","metadata":{"id":"E962441467F24FAA8DCE3360DFC5D3F3","notebookId":"6476be27ba5c1a7de46f2a36","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"data_3d.dtypes","outputs":[{"output_type":"execute_result","data":{"text/plain":"permno                   int64\nret                    float64\nexchcd                 float64\nshrcd                  float64\ndate            datetime64[ns]\n                     ...      \nrank_cinvest           float64\nrank_op                float64\nrank_agr               float64\nrank_ep                float64\nlog_me                 float64\nLength: 61, dtype: object"},"metadata":{},"execution_count":58}],"execution_count":58},{"cell_type":"code","metadata":{"id":"6947847AFF0447C58A433A16A80A6C3F","notebookId":"6476be27ba5c1a7de46f2a36","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"basic_2_factors = Factor_models(data,20,freq='m') #2013年只有八年的数据","outputs":[],"execution_count":95},{"cell_type":"code","metadata":{"id":"98DF445833CA4AEC874325D53810BE7B","notebookId":"6476be27ba5c1a7de46f2a36","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"# 计算样本外R2，运行耗时较长\nroos = basic_2_factors.cal_oos() # 计算不同模型样本外R2。self.cal_oos()中已经包含了self.predict_ret()的操作，先通过不同的模型预测收益率，再比较样本外真实收益率和预测收益率的差异\nroos.to_csv('/home/mw/project/nn3_tuning_roos.csv')","outputs":[],"execution_count":1},{"cell_type":"code","metadata":{"id":"3DA5267B7682431E85C9B82ED28F5D8D","notebookId":"6476be27ba5c1a7de46f2a36","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"# 查看不同模型在不同月份的样本外预测ret\npredict = basic_2_factors._preddf\npredict.to_csv('/home/mw/project/AEencoder_predict.csv')","outputs":[],"execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python","nbconvert_exporter":"python","file_extension":".py","version":"3.5.2","pygments_lexer":"ipython3"}},"nbformat":4,"nbformat_minor":0}
{"cells":[{"cell_type":"markdown","metadata":{"id":"BA4248F432AE40A4BA3715D297D6B69A","trusted":true,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":true,"runtime":{"status":"default","execution_status":null},"scrolled":false,"notebookId":"64896c9ea44b729e44197ba9"},"source":"# StockProject NN3"},{"cell_type":"markdown","metadata":{"id":"EABE87411EE9477188D4F3354F9A1115","notebookId":"64896c9ea44b729e44197ba9","runtime":{"status":"default","execution_status":null},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"}},"source":"- StockProject旨在使用股票市场过去的历史数据，对未来的股票收益率（return）进行尽可能精确的预测。  \n- 本文档使用的模型NN3。  \n- 搭建神经网络使用的框架为`tensorflow,keras`。  \n- 使用GPU训练神经网络，请注意将资源调整为GPU资源"},{"cell_type":"markdown","metadata":{"id":"E9F17E5A8BA5415C9A84392CF6BA34E4","notebookId":"64896c9ea44b729e44197ba9","runtime":{"status":"default","execution_status":null},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"}},"source":"# Package"},{"cell_type":"code","metadata":{"id":"1C94D641011F4F8D980D1A350B623CB7","trusted":true,"collapsed":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"scrolled":false,"notebookId":"64896c9ea44b729e44197ba9"},"source":"#数据读取\nimport os\nimport pyarrow.feather as feather\n\n#数据处理\nimport pandas as pd\nimport numpy as np\n\n\n#进程展示\nfrom tqdm import tqdm\n\n#sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\n\n#NN\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n#超参优化\nimport optuna\n\n#存贮模型\nimport pickle\n\n#作图\nimport matplotlib.pyplot as plt ","outputs":[],"execution_count":2},{"cell_type":"markdown","metadata":{"id":"541916D569B84A8782B8FB9B9B00EB10","notebookId":"64896c9ea44b729e44197ba9","runtime":{"status":"default","execution_status":null},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"}},"source":"# 配置GPU进行训练"},{"cell_type":"code","metadata":{"id":"7EE5C7DFBF624068984951A3DBEC2276","notebookId":"64896c9ea44b729e44197ba9","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"# Specify the GPU device to use\nphysical_devices = tf.config.list_physical_devices('GPU')\ntf.config.experimental.set_memory_growth(physical_devices[0], True)","outputs":[],"execution_count":1},{"cell_type":"markdown","metadata":{"id":"CBE32B6FE8EB49898E220474A801A397","notebookId":"64896c9ea44b729e44197ba9","runtime":{"status":"default","execution_status":null},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"}},"source":"# 数据预处理"},{"cell_type":"markdown","metadata":{"id":"2F3B1663E4084999960A1CB59F6C92B4","notebookId":"64896c9ea44b729e44197ba9","runtime":{"status":"default","execution_status":null},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"}},"source":" 导入处理后的WRDS股票数据`/home/mw/input/stock3636/chars60_rank_imputed.feather`，并进行简单的数据预处理：  \n -  通过滞后一期，让当期的变量中包含需预测的变量（下一期的回报率）  \n -  删除分类变量（通过emedding和label encoder发现收效甚微）"},{"cell_type":"code","metadata":{"id":"A8F3224A46614DAC87E5BF113F3C3050","notebookId":"64896c9ea44b729e44197ba9","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"# 设置全局随机种子，以保证结果可重复性\ntf.random.set_seed(42)\nnp.random.seed(42)","outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"id":"0E5BC0D778D044F290FB369C2AD731B5","trusted":true,"collapsed":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"scrolled":false,"notebookId":"64896c9ea44b729e44197ba9"},"source":"#导入数据\nwith open('/home/mw/input/stock3636/chars60_rank_imputed.feather', 'rb') as f:\n    data = feather.read_feather(f)\ndata['date'] = data['date'].astype('datetime64')\n\n# 滞后代码\n#我们的预测变量为下一期的股票收益率，需将下一期的股票收益率挪至本期\ndata['year_month'] = pd.to_datetime(data['date']).dt.to_period('M')\ndata['ret_fut'] = data.groupby('permno')['ret'].shift(-1)\ndata = data.dropna(axis=0,subset=['ret_fut']) #删除有空缺值的行\n\ndata.set_index('date', inplace=True)\n\n# 缺失值处理\n## 查看缺失值--没有缺失值\nprint('Missing data: {} items\\n'.format(len(data[data.isna().any(1)])), data[data.isna().any(1)]) # 看一下缺失值是哪些行\n\n#删除多多余的变量\n\n#删除分类变量--embedding后约等于没有作用且速度慢\ns = (data.dtypes == 'int64')\nobject_cols = list(s[s].index)# 移除含有类别变量的列\n# 移除数据集含有类别变量的列\ndata = data.drop(object_cols, axis=1)\n\n#删除影响数据分析的变量\ndata = data.drop(['rank_mom36m','rank_mom60m','exchcd','shrcd','lag_me','log_me'],axis=1)","outputs":[],"execution_count":1},{"cell_type":"markdown","metadata":{"id":"6EA0C6F8DF4C4BD0A6F294A9EEF3E1C4","notebookId":"64896c9ea44b729e44197ba9","runtime":{"status":"default","execution_status":null},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"}},"source":"# 模型 NN3"},{"cell_type":"markdown","metadata":{"id":"135931062AC44195B0D4B08B0D72C372","notebookId":"64896c9ea44b729e44197ba9","runtime":{"status":"default","execution_status":null},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"}},"source":"定义了类`Factor_models` ，该类旨在实现预测股票收益率并计使用样本外R方评估模型：  \n\n$R_{OOS}^2 =  1 - \\frac{\\sum_{it} (ret_{it} -\\hat{ret}^2_{it}) }{\\sum_{it} ret_{it}^2}$  \n\n这里$\\hat{ret}_{it}$代表模型预测的第$i$只股票在$t$时期的收益率（return）  \n\n- 在类的初始化方法 `__init__` 中，需要传入股票数据 `data`、初始训练时期长度 `train_period `和训练集扩展的频率 `freq`（默认为按月）。该方法用于对类的参数进行初始化。  \n\n- `predict_ret` 方法用于预测股票收益率。其根据数据的时间索引确定训练和测试日期，并**逐月拆分训练集和测试集**。然后，对训练数据进行标准化处理，使用`NN3`对训练集进行拟合，并预测测试集的收益率。同时，每**12个月调整一次超参**，使用**训练集的最后两个月作为超参调整中的验证集**。预测结果和真实值被存储在一个数据帧中，并在每个训练结束日期打印样本外预测的R2指标。最后，将R2指标保存为CSV文件，并返回包含预测结果的数据帧。  \n```\n #超参数搜索空间                                                        \n# 定义超参  \nnum_layers = trial.suggest_int('num_layers', 3,3)  \nactivation = trial.suggest_categorical('activation', ['relu', 'sigmoid'])  \nnum_neurons = [trial.suggest_int(f'num_neurons_layer_{i}', 8, 256) for i in range(num_layers)]  \nl1s = [trial.suggest_int(f'l1s_{i}',1e-5 , 1e-3) for i in range(num_layers)]  \nlearning_rate = trial.suggest_loguniform('learning_rate', 1e-3, 1e-2)  \ndropout_rate = trial.suggest_float('dropout_rate', 0, 0.5)  \n}                              \n```\n\n- 使用GPU训练神经网络：  \n```\nwith tf.device('/GPU:0'):  \n\t\t#训练模型  \n\t\tmodel.fit(X_train, y_train, batch_size=batch_size, epochs=epochs,validation_data=(X_valid, y_valid), callbacks=[early_stopping],verbose=0  \n```\n\n- `cal_oos` 方法用于计算样本外的R2指标。它首先检查是否已经运行过 `predict_ret` 方法，如果是，则直接使用已有的预测结果数据帧，否则先调用` predict_ret` 方法进行预测。然后，根据预测结果计算样本外的R2指标，并绘制不同模型的样本外R2柱状图。最后，返回包含不同模型样本外R2指标的数据系列。  \n- 模型存储在地址`/home/mw/project/recording/NN3.pkl`  \n"},{"cell_type":"code","metadata":{"id":"82F83AAE45DD4799AB34669AFA501987","notebookId":"64896c9ea44b729e44197ba9","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"class Factor_models(object):\n\n    def __init__(self,data,train_period,freq='m'):\n      \n       #参数初始化\n        self.data = data\n        self.train_period = train_period #初始训练时期长度\n        self.freq = freq                 #训练集expanding的频率，是按月还是按年还是其他\n        \n    def predict_ret(self):\n        dates = self.data.index.unique()\n        dates = dates.sort_values()\n        print(\"=====dates=======\")\n        print(dates)\n        test_dates= dates[self.train_period:len(dates)]\n        print(\"====test months=====\")\n        print(test_dates)\n        \n        # 创建一个train_end_list，训练集每月expanding。\n        preddf = pd.DataFrame() # 存储不同模型预测出来的y值，即存储样本外预测收益率的值\n        #记录R方\n        R2df = pd.DataFrame() # 存储不同模型预测出来的y值，即存储样本外预测收益率的值\n        #记录超参变化\n        best_hp = pd.DataFrame()\n\n        index = 0  # 计数用\n        tuning_index = 0 #调参记数用 \n        for end_date in tqdm(test_dates,desc='Spilt and Train'): # 通过逐月改变训练集end_date的方法，切割样本\n            \n            #训练用数据\n            train_temp = self.data[self.data.index <  end_date]\n            test_temp = self.data[self.data.index == end_date]\n            \n            #测试集\n            y_test = test_temp.ret_fut\n            X_test = test_temp.drop(['ret_fut','year_month'], axis=1)\n\n            #预测训练数据集\n            y_ptrain = train_temp.ret_fut\n            X_ptrain = train_temp.drop(['ret_fut','year_month'], axis=1)\n            \n            #对数据进行逐列标准化\n            s = (X_ptrain.dtypes == 'float64')\n            object_cols = list(s[s].index)\n\n            scaler = StandardScaler()\n            for col in object_cols:\n                scaler.fit(X_test[col].values.reshape(-1,1))\n                X_test[col] = scaler.transform(X_test[col].values.reshape(-1,1))\n                scaler.fit(X_ptrain[col].values.reshape(-1,1))\n                X_ptrain[col] = scaler.transform(X_ptrain[col].values.reshape(-1,1))\n    \n            # 建模预测收益率\n            ## 先创建一个临时的temp_preddf,用来存储当前月份的验证集下的real y和不同模型的预测y\n            temp_preddf = pd.DataFrame() # 创建当前训练集下训练出的predict y和real y\n            y_test_rec = y_test.values.reshape(-1,1) #转为numpy\n            temp_preddf['real_y'] = y_test_rec[:,0]# real_y就是验证集valid_y的第一列。因为valid_y是真实收益率数据在vailid_date上的切割\n         \n            \"\"\"\n            超参调整\n            \"\"\" \n\n            if index == 0 or index % 12 == 11:\n\n                #tuning使用数据集\n                end_del2 = end_date - np.timedelta64(2,'M')\n                print('原日期：', end_date)\n                print('向前推两个月后的日期：', end_del2)\n                temp = self.data[(self.data.index <  end_del2)]\n                valid_temp = self.data[ (self.data.index >= end_del2) & (self.data.index < end_date) ]\n\n\n                #训练集\n                y_train = temp.ret_fut\n                X_train = temp.drop(['ret_fut','year_month'], axis=1)\n    \n\n                # 验证集\n                y_valid = valid_temp.ret_fut\n                X_valid = valid_temp.drop(['ret_fut','year_month'], axis=1)\n                \n                #数据标准化\n                scaler = StandardScaler()\n                for col in object_cols:\n                    scaler.fit(X_train[col].values.reshape(-1,1))\n                    X_train[col] = scaler.fit_transform(X_train[col].values.reshape(-1,1))\n                    scaler.fit(X_valid[col].values.reshape(-1,1))\n                    X_valid[col] = scaler.fit_transform(X_valid[col].values.reshape(-1,1))\n    \n\n\n                # 定义optuna使用的目标\n            \n               # 定义目标函数\n                with tf.device('/GPU:0'):\n                    def objective(trial):\n                        # 定义超参\n                        num_layers = trial.suggest_int('num_layers', 3,3)\n                        activation = trial.suggest_categorical('activation', ['relu', 'sigmoid'])\n                        num_neurons = [trial.suggest_int(f'num_neurons_layer_{i}', 8, 256) for i in range(num_layers)]\n                        l1s = [trial.suggest_int(f'l1s_{i}',1e-5 , 1e-3) for i in range(num_layers)]\n                        learning_rate = trial.suggest_loguniform('learning_rate', 1e-3, 1e-2)\n                        dropout_rate = trial.suggest_float('dropout_rate', 0, 0.5)\n\n\n                        # 构建网络\n                        model = keras.Sequential()\n                        model.add(layers.Dense(num_neurons[0], activation=activation, input_shape=(53,)))\n                        for i in range(1, num_layers):\n                            model.add(layers.Dense(num_neurons[i], activation=activation, kernel_regularizer=regularizers.l1(l1s[i])))\n                            model.add(layers.Dropout(dropout_rate))\n                        model.add(layers.Dense(1))\n\n                        # MSE loss和 Adam优化器\n                        model.compile(loss='mse', optimizer=keras.optimizers.Adam(learning_rate=learning_rate))\n\n                        # 定义训练参数\n                        batch_size = 100000\n                        epochs = 100\n\n                        # 定义early stopping\n                        early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n\n                        #训练模型\n                        model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs,validation_data=(X_valid, y_valid), callbacks=[early_stopping],verbose=0)\n        \n                        #返回valuation\n                        val_loss = model.evaluate(X_test, y_test)\n                        return val_loss\n                  \n\n                # 创建 Optuna 优化器\n                study = optuna.create_study(direction='minimize')\n\n                # 运行优化器\n                study.optimize(objective, n_trials=10)\n\n                # 最优参数\n                trial = study.best_trial\n\n                #将最优超参数记录到数据帧中\n                best_hp = best_hp.append(study.best_trial.params, ignore_index=True)\n            \n            # NN模型及训练\n            with tf.device('/GPU:0'):\n                model = keras.Sequential()\n                model.add(layers.Dense(trial.params[\"num_neurons_layer_0\"], activation=trial.params[\"activation\"], input_shape=(53,)))\n                for i in range(1, trial.params[\"num_layers\"]):\n                    model.add(layers.Dense(trial.params[f\"num_neurons_layer_{i}\"], activation=trial.params[\"activation\"],kernel_regularizer=regularizers.l1(trial.params[f\"l1s_{i}\"])))\n                    model.add(layers.Dropout(trial.params[\"dropout_rate\"]))\n                model.add(layers.Dense(1))\n\n                model.compile(loss='mse', optimizer=keras.optimizers.Adam(learning_rate=trial.params[\"learning_rate\"]))\n                # 定义训练参数\n                batch_size = 100000\n                epochs = 100\n                # 定义early stopping\n                early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n                model.fit(X_ptrain, y_ptrain,batch_size=batch_size, epochs=epochs, verbose=0)\n                y_predict = model.predict(X_test)\n            \n            ## 将temp_preddf并入preddf\n            temp_preddf['NN3g_y'] = y_predict\n            preddf = preddf.append(temp_preddf) # 将当前valid_date下得到的predict_y和real_y一起并入preddf中\n            self._preddf = preddf\n            \n            #R2\n            denominator = (preddf['real_y'] ** 2).sum() # 分母是真实收益率的平方和\n            numerator = preddf.apply(lambda x: preddf['real_y'] - x).iloc[:,1:] # 分子是real_y - predict_y的平方和\n            numerator = (numerator ** 2).sum()\n            R2 = 1 - numerator / denominator # 再用 1 减去分子/分母\n            print(\"==================\")\n            print(\"date\",end_date)\n            print(\"Out-of-sample predicting R2:\",R2)\n            print(\"==================\")\n            R2df = R2df.append(R2,ignore_index=True )\n            \n            ## 将temp_preddf并入preddf\n            preddf = preddf.append(temp_preddf) # 将当前valid_date下得到的predict_y和real_y一起并入preddf中\n            self._preddf = preddf\n            index += 1\n\n        # 将数据帧保存为 CSV 文件\n        best_hp.to_csv('/home/mw/project/recording/best_params_NN3.csv', index=False)\n        R2df.to_csv('/home/mw/project/recording/R2_NN3.csv', index=False)\n        \n        #保存模型\n        # 将模型保存到磁盘\n        with open('/home/mw/project/recording/NN3.pkl', 'wb') as f:\n            pickle.dump(model, f)\n        \n        return preddf # 最后我们只返回preddf，也就是所有期的predict y和real y\n    \n    def cal_oos(self):\n        # 计算out-of-sample R2 根据代码开头的公式\n        try:\n            preddf = self._preddf # 如果self已经有self._preddf，即self.predict_ret()已经运行过了，已经预测过收益率了，则无需再次运行。\n        except:\n            preddf = self.predict_ret() # 如果之前没有运行过self.predict_ret()，则需要运行。\n        denominator = (preddf['real_y'] ** 2).sum() # 分母是真实收益率的平方和\n        numerator = preddf.apply(lambda x: preddf['real_y'] - x).iloc[:,1:] # 分子是real_y - predict_y的平方和\n        numerator = (numerator ** 2).sum()\n        \n        roos = 1 - numerator / denominator # 再用 1 减去分子/分母\n        roos.index = roos.index.str.rstrip('_y') # 之前的index都是模型_y，比如\"OLS_y\"，不美观，删除_y。\n        fig,ax = plt.subplots(figsize = (16,12)) # 画图，将不同模型的Roos画出来。\n        plt.title('Out-of-sample predicting R2', fontsize = 20)\n        ax.bar(x = roos.index, height = roos)\n        plt.show()\n        return roos # 返回样本外Roos，这个Roos是不同模型对应的样本外R2","outputs":[],"execution_count":48},{"cell_type":"markdown","metadata":{"id":"CF6F579DDCDE4B61B8CB45E0521119EE","notebookId":"64896c9ea44b729e44197ba9","runtime":{"status":"default","execution_status":null},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"}},"source":"# 预测"},{"cell_type":"code","metadata":{"id":"FA07D9E7F68A46B7A50524EDE1CDFAD2","notebookId":"64896c9ea44b729e44197ba9","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"basic_2_factors = Factor_models(data,32,freq='m') #2013年只有8个月","outputs":[],"execution_count":49},{"cell_type":"code","metadata":{"id":"A535CFD861084BB0AFFFFC96B0C771FD","notebookId":"64896c9ea44b729e44197ba9","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"# 计算样本外R2，运行耗时较长\nroos = basic_2_factors.cal_oos() # 计算不同模型样本外R2。self.cal_oos()中已经包含了self.predict_ret()的操作，先通过不同的模型预测收益率，再比较样本外真实收益率和预测收益率的差异\nroos.to_csv('/home/mw/project/nn3.csv')","outputs":[],"execution_count":2}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python","nbconvert_exporter":"python","file_extension":".py","version":"3.5.2","pygments_lexer":"ipython3"}},"nbformat":4,"nbformat_minor":0}